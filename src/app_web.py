import streamlit as st
from openai import OpenAI
from qdrant_client import QdrantClient

# --- 0. GARANTIR A INICIALIZA√á√ÉO DO SESSION STATE ---
# Esta √© a corre√ß√£o principal. N√≥s garantimos que 'db_count'
# sempre exista, mesmo antes de tentar a conex√£o.
if "db_count" not in st.session_state:
    st.session_state.db_count = -1  # Usamos -1 para significar "n√£o verificado"
# ----------------------------------------------------

# --- Constantes ---
NOME_DA_COLECAO = "leis_fiscais_v1"
MODELO_EMBEDDING = st.secrets['MODELO_EMBEDDING']
OPENAI_MODEL=st.secrets['OPENAI_MODEL']


# --- 1. CONFIGURA√á√ÉO (LENDO st.secrets) ---

@st.cache_resource
def carregar_cerebro_e_executor():
    """Conecta ao Qdrant e ao LLM da OpenAI usando st.secrets."""
    print("Conectando aos servi√ßos...")
    try:
        # 1. Conectar ao Qdrant
        qdrant_client = QdrantClient(
            url=st.secrets['QDRANT_URL'], 
            api_key=st.secrets['QDRANT_API_KEY']
        )
        print("‚úÖ C√©rebro (Qdrant Cloud) carregado.")

        # 2. Conectar ao LLM
        llm_client = OpenAI(
            api_key=st.secrets['OPENAI_API_KEY']
        )
        print("‚úÖ Executor (OpenAI LLM) conectado.")

        # 3. Verificar contagem (s√≥ se as conex√µes funcionarem)
        try:
            count = qdrant_client.count(collection_name=NOME_DA_COLECAO, exact=True)
            st.session_state.db_count = count.count
        except Exception as e:
            st.session_state.db_count = 0 # DB conectado, mas cole√ß√£o vazia
            st.error(f"Cole√ß√£o '{NOME_DA_COLECAO}' n√£o encontrada no Qdrant! Voc√™ 'encheu' o C√©rebro na Nuvem?")
            print(f"Erro Qdrant: {e}")

        return qdrant_client, llm_client
    
    except KeyError as e:
        st.error(f"Erro: A 'Secret' {e} n√£o foi definida no painel do Streamlit Cloud!")
        st.session_state.db_count = -2 # C√≥digo de erro: Secret faltando
        return None, None
    except Exception as e:
        print(f"‚ùå Erro na inicializa√ß√£o: {e}")
        st.error(f"Erro fatal ao conectar aos servi√ßos: {e}")
        st.session_state.db_count = -3 # C√≥digo de erro: Outra falha de conex√£o
        return None, None

# Carrega os servi√ßos
qdrant_client, llm_client = carregar_cerebro_e_executor()


# --- 2. INTERFACE WEB (STREAMLIT) ---

st.set_page_config(layout="wide")
st.title("ü§ñ Agente Fiscal v2.0 (Qdrant Engine)")
st.markdown(f"Alimentado com a **EC 132** e **LC 214**. Fatias no C√©rebro: **{st.session_state.get('db_count', 0)}**")

col1, col2 = st.columns(2)

with col1:
    st.subheader("1. Perfil do Cliente")
    perfil_cliente = st.text_area(
        "Descreva a empresa:",
        height=150,
        value="""{
"nome_empresa": "Construtora Alfa Ltda",
"cnae_principal": "4120-4/00 (Constru√ß√£o de Edif√≠cios)",
"regime_tributario": "Simples Nacional",
"faturamento_anual": "R$ 3.000.000,00"
}"""
    )
    
    st.subheader("2. Pergunta Espec√≠fica")
    pergunta_cliente = st.text_input(
        "Fa√ßa sua pergunta:",
        value="Eu terei direito ao cr√©dito de IBS e CBS?"
    )

    run_button = st.button("Executar An√°lise", type="primary")

with col2:
    st.subheader("3. Resposta do Agente")
    
    # --- L√ìGICA DE EXIBI√á√ÉO CORRIGIDA ---
    # N√≥s reestruturamos este bloco 'if/else' para ser mais claro
    # e para usar o m√©todo .get() que √© mais seguro.
    
    db_count = st.session_state.get('db_count', -1) # Pega o valor com seguran√ßa

    if run_button:
        # O bot√£o FOI clicado
        if llm_client and qdrant_client and db_count > 0:
            with st.spinner("Analisando... (Isso pode levar at√© 30 segundos)"):
                try:
                    # --- 3. L√ìGICA RAG (QDRANT) ---
                    print("Iniciando consulta RAG...")
                    query_text = f"Perfil: {perfil_cliente}\nPergunta: {pergunta_cliente}"
                    
                    print("Criando vetor para a pergunta...")
                    embedding_response = llm_client.embeddings.create(
                        input=query_text,
                        model=MODELO_EMBEDDING
                    )
                    query_vector = embedding_response.data[0].embedding
                    
                    print("Buscando no Qdrant...")
                    resultados = qdrant_client.search(
                        collection_name=NOME_DA_COLECAO,
                        query_vector=query_vector,
                        limit=7
                    )
                    
                    contexto_juridico = "\n---\n".join(
                        [hit.payload['texto'] for hit in resultados]
                    )
                    print(f"Contexto RAG recuperado ({len(resultados)} fatias).")

                    # --- ETAPA DE GERA√á√ÉO (LLM) ---
                    PROMPT_MESTRE = """
                    Voc√™ √© o "IA Fiscal Advisor", um consultor tribut√°rio S√™nior.
                    Responda a pergunta do cliente com base *exclusivamente* no Perfil do Cliente e no Contexto Jur√≠dico (fatias das leis) fornecido.
                    Seja direto, claro e cite os artigos ou se√ß√µes do contexto que fundamentam sua resposta.
                    """
                    
                    prompt_usuario = f"""
                    **Perfil do Cliente:**
                    {perfil_cliente}

                    **Pergunta do Cliente:**
                    "{pergunta_cliente}"

                    **Contexto Jur√≠dico Recuperado da Base (Use APENAS isso):**
                    ---
                    {contexto_juridico}
                    ---

                    **Sua Resposta (seja direto e fundamente no contexto):**
                    """
                    
                    print("Enviando para o LLM...")
                    completion = llm_client.chat.completions.create(
                        model=OPENAI_MODEL,
                        temperature=0.0,
                        messages=[
                            {"role": "system", "content": PROMPT_MESTRE},
                            {"role": "user", "content": prompt_usuario}
                        ]
                    )
                    
                    resposta_final = completion.choices[0].message.content
                    st.markdown(resposta_final)
                    
                    with st.expander("Ver fontes (payloads) usadas pelo RAG"):
                        st.json([hit.payload for hit in resultados])

                except Exception as e:
                    st.error(f"Erro durante a execu√ß√£o: {e}")
        
        elif db_count == 0:
            st.error("O C√©rebro (Qdrant) est√° vazio. Verifique se o 'processador.py' foi executado corretamente.")
        else:
            # Se chegou aqui, √© porque llm_client ou qdrant_client falharam na conex√£o
            st.error("Erro de conex√£o. Verifique suas 'Secrets' no painel do Streamlit Cloud e recarregue a p√°gina.")
    
    else:
        # O bot√£o N√ÉO foi clicado. A p√°gina est√° apenas esperando.
        st.info("Preencha o perfil e a pergunta, depois clique em 'Executar An√°lise'.")